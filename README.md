# ECTransformer : A hierarchical GPU scheduling Framework for Transformer inference in edge computing 

Deep learning has been widely used in many applications. However, deep learning applications are generally computation-intensive and memory-intensive, so it is difficult to deploy on edge devices with limited resources.. Most of the current research focuses on distributed systems, job scheduling and model compression, but overlook the key issue of how to make more tasks run simultaneously on edge devices. In this paper, we propose ECTransformer, A hierarchical GPU scheduling Framework for Transformer inference in edge computing. In this paper, we analyze the characteristics of transformer model and the time cost on CPU and GPU. Guided by profiling, ECTransformer designs an edge-device partition method to reduce resource overhead on edge nodes.Ulteriorly, We also propose the deployment scheduling framework on the edge server side.By shared memory, scheduling queues and pipelinesï¼Œthe problems of high memory resource overhead and poor real-time performance of Transformer tasks on edge nodes are solved. Experiment results show that our proposed pipeline scheme saves 15-25 % of memory and speeds up 10 % to the conventional solution. When combined, ECTransformer can save up to 25% on GPU memory overhead and improve 5.1x processing efficiency.

